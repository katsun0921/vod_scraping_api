import asyncio
import json
from pathlib import Path
import sys

SPIDERS = {
    "netflix_spider": "netflix.json",
    "primevideo_spider": "primevideo.json",
    "unext_spider": "unext.json",
}

OUTPUT_DIR = Path("outputs")
SUMMARY_FILE = OUTPUT_DIR / "vod_summary.json"

async def run_spider(spider_name: str, output_file: str):
    """Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’éåŒæœŸã§å®Ÿè¡Œã—ã€å€‹åˆ¥JSONã«å‡ºåŠ›"""
    print(f"\nğŸš€ Running spider: {spider_name}")

    process = await asyncio.create_subprocess_exec(
        "scrapy", "crawl", spider_name,
        f"-o={OUTPUT_DIR / (output_file + ':jsonlines')}",
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await process.communicate()

    stdout = stdout.decode("utf-8", errors="ignore")
    stderr = stderr.decode("utf-8", errors="ignore")

    print(f"--- {spider_name} output ---")
    print(stdout)
    if stderr:
        print(f"âš ï¸ {spider_name} error:\n{stderr}", file=sys.stderr)

    if process.returncode == 0:
        print(f"âœ… {spider_name} completed successfully.")
    else:
        print(f"âŒ {spider_name} failed with code {process.returncode}")

    return process.returncode


def read_json_lines(file_path: Path):
    """Scrapyã®JSON Lineså‡ºåŠ›ã‚’å®‰å…¨ã«èª­ã¿è¾¼ã‚€"""
    items = []
    try:
        with file_path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    items.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ Skipping invalid JSON line in {file_path.name}: {e}")
    except FileNotFoundError:
        print(f"âš ï¸ {file_path.name} not found.")
    return items

def load_existing_summary():
    """æ—¢å­˜ã®vod_summary.jsonã‚’èª­ã¿è¾¼ã¿"""
    if SUMMARY_FILE.exists():
        try:
            with SUMMARY_FILE.open("r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            print("âš ï¸ vod_summary.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®šã‚’ç„¡åŠ¹åŒ–ã—ã¾ã™ã€‚")
    return {}


def should_skip_spider(spider_name: str, summary_data: dict) -> bool:
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«æœ‰åŠ¹ãªVODæƒ…å ±ãŒã‚ã‚Œã°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—"""
    service = spider_name.replace("_spider", "")
    for slug, info in summary_data.items():
        vod_info = info.get(service, {})
        if vod_info.get("service") and vod_info["service"] != "unavailable":
            print(f"â© {spider_name} skipped: '{slug}' already has {service} data.")
            return True
    return False

def merge_json_files():
    """å€‹åˆ¥ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®å‡ºåŠ›ã‚’çµ±åˆã—ã¦vod_summary.jsonã‚’ç”Ÿæˆ"""
    print("\nğŸ”„ Merging JSON results...")
    merged = {}
    OUTPUT_DIR.mkdir(exist_ok=True)

    for spider_name, filename in SPIDERS.items():
        file_path = OUTPUT_DIR / filename
        data = read_json_lines(file_path)
        if not data:
            print(f"âš ï¸ No valid data found in {filename}.")
            continue

        for item in data:
            slug = item["slug"]
            title = item["title"]
            service = filename.replace(".json", "")
            if slug not in merged:
                merged[slug] = {
                    "title": title,
                    "netflix": {"url": None, "service": "unavailable", "price": None},
                    "primevideo": {"url": None, "service": "unavailable", "price": None},
                    "unext": {"url": None, "service": "unavailable", "price": None},
                }
            merged[slug][service] = {
                "url": item.get("url"),
                "service": item.get("service"),
                "price": item.get("price"),
            }

    with SUMMARY_FILE.open("w", encoding="utf-8") as f:
        json.dump(merged, f, ensure_ascii=False, indent=2)

    print(f"âœ… çµ±åˆå®Œäº†: {SUMMARY_FILE.resolve()}\n")


async def main():
    print("=== Scrapy Multi-Spider Runner (async + skip + merge) ===\n")

    summary_data = load_existing_summary()
    if SUMMARY_FILE.exists():
        backup = SUMMARY_FILE.with_name("vod_summary_backup.json")
        SUMMARY_FILE.rename(backup)
        print(f"ğŸ“¦ Backup created: {backup}")

    # ã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦å®Ÿè¡Œå¯¾è±¡ã®ã¿é¸æŠ
    spiders_to_run = [
        (name, file)
        for name, file in SPIDERS.items()
        if not should_skip_spider(name, summary_data)
    ]

    if not spiders_to_run:
        print("â¹ ã™ã¹ã¦ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
    else:
        results = await asyncio.gather(*(run_spider(name, file) for name, file in spiders_to_run))
        if all(r == 0 for r in results):
            print("\nğŸ‰ All executed spiders finished successfully.")
        else:
            print("\nâš ï¸ Some spiders failed. Check logs above.")

    merge_json_files()


if __name__ == "__main__":
    asyncio.run(main())
